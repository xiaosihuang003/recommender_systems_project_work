{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1408dd3-7acb-4265-89fa-8615e86a6828",
   "metadata": {},
   "source": [
    "# Part II: SDAA (Satisfaction and Disagreement Aware Aggregation) (25 points)\n",
    "\n",
    "**Students:** Oskari Perikangas, Xiaosi Huang  \n",
    "**Date:** November 10, 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522d72d8-af68-4621-a10c-17267828413d",
   "metadata": {},
   "source": [
    "# Step 1: Design - Sequential Group Recommendation (7 points)\n",
    "\n",
    "### Method: SDAA from SQUIRREL Framework\n",
    "\n",
    "SQUIRREL (Sequential Group Recommendation with Reinforcement Learning) is a framework that uses multiple aggregation methods as actions. We implement and compare three key methods from this framework:\n",
    "\n",
    "1. **Average** - Baseline method treating all users equally\n",
    "2. **Least Misery** - Focuses on the least satisfied user  \n",
    "3. **SDAA (Satisfaction and Disagreement Aware Aggregation)** - main method\n",
    "\n",
    "All three methods share the same **SQUIRREL state representation and reward function**, but differ in their aggregation strategies.\n",
    "\n",
    "---\n",
    "\n",
    "## SDAA Design\n",
    "\n",
    "SDAA dynamically adjusts user weights based on cumulative satisfaction history to ensure fairness across multiple rounds.\n",
    "\n",
    "#### Key Formulas\n",
    "\n",
    "1. **Overall Satisfaction** (State in SQUIRREL)\n",
    "   $$\\text{satO}(u, RS) = \\frac{1}{\\mu} \\sum_{j=1}^{\\mu} \\text{sat}(u, Gr_j)$$\n",
    "\n",
    "2. **Fairness Weight** (Our modification)\n",
    "   $$w(u) = 1 - \\frac{\\text{satO}(u) - \\min_u \\text{satO}(u)}{\\max_u \\text{satO}(u) - \\min_u \\text{satO}(u)}$$\n",
    "   Lower satisfaction → Higher weight\n",
    "\n",
    "3. **Alpha (Satisfaction Disparity)**\n",
    "   $$\\alpha_j = \\max_{u \\in G} \\text{sat}(u, Gr_{j-1}) - \\min_{u \\in G} \\text{sat}(u, Gr_{j-1})$$\n",
    "\n",
    "4. **SDAA Aggregation**\n",
    "   $$\\text{score}(G, i, j) = (1 - \\alpha_j) \\cdot \\text{weighted\\_avg}(G, i, j) + \\alpha_j \\cdot \\text{least}(G, i, j)$$\n",
    "\n",
    "5. **Reward Function** (SQUIRREL R_sd)\n",
    "   $$R_{sd}(RS_j) = \\frac{2 \\cdot \\text{groupSatO}(RS_j) \\cdot (1 - \\text{groupDisO}(RS_j))}{\\text{groupSatO}(RS_j) + (1 - \\text{groupDisO}(RS_j))}$$\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Steps\n",
    "\n",
    "1. Track satisfaction history for each user across rounds (SQUIRREL state)\n",
    "2. Calculate cumulative satisfaction and fairness weights\n",
    "3. Implement three aggregation methods: Average, Least Misery, SDAA\n",
    "4. Blend weighted average with least misery using alpha (for SDAA)\n",
    "5. Select top-k items and update state\n",
    "6. Calculate R_sd reward to evaluate performance\n",
    "\n",
    "---\n",
    "\n",
    "Based on the SQUIRREL framework, we:\n",
    "- Implemented three aggregation methods for comparison\n",
    "- Added **fairness weighting** to SDAA based on cumulative satisfaction\n",
    "- This ensures users with consistently low satisfaction get priority in future rounds\n",
    "- Used SQUIRREL's R_sd reward function to evaluate all methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a821f-5125-4c7c-bb46-3da78be27a0c",
   "metadata": {},
   "source": [
    "# Step 2: Implementation (7 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3875623f-b4af-4139-9be2-9ae39d1a0faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading MovieLens Dataset ===\n",
      "Loaded: 610 users, 100836 ratings\n",
      "Test group: [1, 414, 599]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== Loading MovieLens Dataset ===\")\n",
    "\n",
    "# Load ratings\n",
    "ratings_df = pd.read_csv(\"../data/ml-latest-small/ratings.csv\")\n",
    "\n",
    "if 'userId' in ratings_df.columns:\n",
    "    ratings_df.rename(columns={'userId': 'user_id', 'movieId': 'item_id'}, inplace=True)\n",
    "\n",
    "# Create user ratings dictionary\n",
    "user_ratings_dict = {}\n",
    "for user_id in ratings_df['user_id'].unique():\n",
    "    user_ratings_dict[user_id] = {}\n",
    "    user_data = ratings_df[ratings_df['user_id'] == user_id]\n",
    "    for _, row in user_data.iterrows():\n",
    "        user_ratings_dict[user_id][row['item_id']] = row['rating']\n",
    "\n",
    "print(f\"Loaded: {len(user_ratings_dict)} users, {len(ratings_df)} ratings\")\n",
    "\n",
    "# Select test group with diverse preferences\n",
    "test_group = [1, 414, 599]\n",
    "print(f\"Test group: {test_group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b46be2-f071-43fb-a4e6-bd60e12ab5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_ratings=20: 1297 movies (13.3% of total)\n",
      "min_ratings=30: 882 movies (9.1% of total)\n",
      "min_ratings=50: 450 movies (4.6% of total)\n",
      "min_ratings=75: 236 movies (2.4% of total)\n",
      "min_ratings=100: 138 movies (1.4% of total)\n",
      "User 1: 232 rated movies, 158 overlap with threshold=30 (68.1%), 117 overlap with threshold=50 (50.4%)\n",
      "User 414: 2698 rated movies, 809 overlap with threshold=30 (30.0%), 429 overlap with threshold=50 (15.9%)\n",
      "User 599: 2478 rated movies, 703 overlap with threshold=30 (28.4%), 388 overlap with threshold=50 (15.7%)\n"
     ]
    }
   ],
   "source": [
    "def compare_popularity_thresholds():\n",
    "    \"\"\"Compare different popularity thresholds for movie selection\"\"\"\n",
    "    movie_counts = ratings_df.groupby('item_id').size()\n",
    "    \n",
    "    # Analyze different popularity thresholds\n",
    "    thresholds = [20, 30, 50, 75, 100]\n",
    "    for threshold in thresholds:\n",
    "        count = len(movie_counts[movie_counts >= threshold])\n",
    "        percentage = (count / len(movie_counts)) * 100\n",
    "        print(f\"min_ratings={threshold}: {count} movies ({percentage:.1f}% of total)\")\n",
    "    \n",
    "    # Analyze rating coverage for our test users\n",
    "    test_group = [1, 414, 599]\n",
    "    for user_id in test_group:\n",
    "        user_rated = set(ratings_df[ratings_df['user_id'] == user_id]['item_id'])\n",
    "        popular_30 = set(movie_counts[movie_counts >= 30].index)\n",
    "        popular_50 = set(movie_counts[movie_counts >= 50].index)\n",
    "        \n",
    "        overlap_30 = len(user_rated & popular_30)\n",
    "        overlap_50 = len(user_rated & popular_50)\n",
    "        \n",
    "        print(f\"User {user_id}: {len(user_rated)} rated movies, \"\n",
    "              f\"{overlap_30} overlap with threshold=30 ({overlap_30/len(user_rated)*100:.1f}%), \"\n",
    "              f\"{overlap_50} overlap with threshold=50 ({overlap_50/len(user_rated)*100:.1f}%)\")\n",
    "\n",
    "# Call the comparison function\n",
    "compare_popularity_thresholds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43264e42-6e8e-4730-8fd7-c6e696b135cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupState:\n",
    "    \"\"\"Manages group recommendation state\"\"\"\n",
    "    \n",
    "    def __init__(self, group_users, user_ratings_dict):\n",
    "        self.group_users = group_users\n",
    "        self.user_ratings_dict = user_ratings_dict\n",
    "        self.sat_history = {user: [] for user in group_users}\n",
    "        self.recommendation_history = []\n",
    "        self.current_round = 0\n",
    "        \n",
    "    def get_alpha(self):\n",
    "        \"\"\"Calculate alpha_j from previous round satisfaction disparity\"\"\"\n",
    "        if self.current_round == 0:\n",
    "            return 0.5\n",
    "        \n",
    "        prev_sats = []\n",
    "        for user in self.group_users:\n",
    "            if len(self.sat_history[user]) >= self.current_round:\n",
    "                prev_sats.append(self.sat_history[user][self.current_round - 1])\n",
    "        \n",
    "        if not prev_sats:\n",
    "            return 0.5\n",
    "        \n",
    "        alpha = max(prev_sats) - min(prev_sats)\n",
    "        return max(0.0, min(1.0, alpha))\n",
    "    \n",
    "    def update_round(self, recommendations, satisfactions):\n",
    "        \"\"\"Update state after each round\"\"\"\n",
    "        self.current_round += 1\n",
    "        self.recommendation_history.append(recommendations)\n",
    "        for user in self.group_users:\n",
    "            if user in satisfactions:\n",
    "                self.sat_history[user].append(satisfactions[user])\n",
    "    \n",
    "    def get_overall_satisfaction(self):\n",
    "        \"\"\"Calculate average satisfaction across all rounds for each user\"\"\"\n",
    "        satO = {}\n",
    "        for user in self.group_users:\n",
    "            if self.sat_history[user]:\n",
    "                satO[user] = np.mean(self.sat_history[user])\n",
    "            else:\n",
    "                satO[user] = 0.0\n",
    "        return satO\n",
    "    \n",
    "    def get_group_satisfaction(self):\n",
    "        \"\"\"Average satisfaction across all users\"\"\"\n",
    "        satO = self.get_overall_satisfaction()\n",
    "        return np.mean(list(satO.values())) if satO else 0.0\n",
    "    \n",
    "    def get_group_disagreement(self):\n",
    "        \"\"\"Max - min satisfaction across users\"\"\"\n",
    "        satO = self.get_overall_satisfaction()\n",
    "        if not satO:\n",
    "            return 0.0\n",
    "        return max(satO.values()) - min(satO.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e9cb7e5-9e93-4d09-aa6f-f9bf4e8fb20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Rating Scale: 0.5 - 5.0\n",
    "# 2. Satisfaction Scale: 0.0 - 1.0\n",
    "\n",
    "class GroupRecommender:\n",
    "    \"\"\"Group recommendation system with Average+LeastMisery+SDAA  aggregation methods\"\"\"\n",
    "    \n",
    "    def __init__(self, group_users, ratings_df, user_ratings_dict, method='SDAA'):\n",
    "        self.state = GroupState(group_users, user_ratings_dict)\n",
    "        self.ratings_df = ratings_df\n",
    "        self.user_ratings_dict = user_ratings_dict\n",
    "        self.method = method\n",
    "        self.popular_movies = self._get_popular_movies()\n",
    "        self.used_movies = set()\n",
    "    \n",
    "    def _get_popular_movies(self, min_ratings=30):\n",
    "        \"\"\"Get movies with enough ratings\"\"\"\n",
    "        movie_counts = self.ratings_df.groupby('item_id').size()\n",
    "        return movie_counts[movie_counts >= min_ratings].index.tolist()\n",
    "    \n",
    "    def _pearson_similarity(self, ratings1, ratings2, min_common=3):\n",
    "        \"\"\"Calculate Pearson correlation\"\"\"\n",
    "        common = set(ratings1.keys()) & set(ratings2.keys())\n",
    "        if len(common) < min_common:\n",
    "            return 0.0\n",
    "        \n",
    "        r1 = [ratings1[i] for i in common]\n",
    "        r2 = [ratings2[i] for i in common]\n",
    "        \n",
    "        m1, m2 = np.mean(r1), np.mean(r2)\n",
    "        num = sum((a - m1) * (b - m2) for a, b in zip(r1, r2))\n",
    "        d1 = np.sqrt(sum((a - m1) ** 2 for a in r1))\n",
    "        d2 = np.sqrt(sum((b - m2) ** 2 for b in r2))\n",
    "        \n",
    "        return num / (d1 * d2) if d1 > 0 and d2 > 0 else 0.0\n",
    "    \n",
    "    def predict_rating(self, user_id, item_id, k=15):\n",
    "        \"\"\"Predict user rating for item using collaborative filtering\"\"\"\n",
    "        if user_id not in self.user_ratings_dict:\n",
    "            return 2.75\n",
    "        \n",
    "        target_ratings = self.user_ratings_dict[user_id]\n",
    "        target_mean = np.mean(list(target_ratings.values()))\n",
    "        \n",
    "        item_ratings = self.ratings_df[self.ratings_df['item_id'] == item_id]\n",
    "        if len(item_ratings) == 0:\n",
    "            return target_mean\n",
    "        \n",
    "        # Find similar users\n",
    "        similarities = []\n",
    "        for _, row in item_ratings.iterrows():\n",
    "            neighbor_id = row['user_id']\n",
    "            if neighbor_id != user_id and neighbor_id in self.user_ratings_dict:\n",
    "                sim = self._pearson_similarity(target_ratings, self.user_ratings_dict[neighbor_id])\n",
    "                if sim > 0:\n",
    "                    similarities.append((sim, neighbor_id, row['rating']))\n",
    "        \n",
    "        if similarities:\n",
    "            similarities.sort(reverse=True)\n",
    "            top_k = similarities[:k]\n",
    "            \n",
    "            weighted_sum = 0.0\n",
    "            sim_sum = 0.0\n",
    "            for sim, neighbor_id, rating in top_k:\n",
    "                neighbor_mean = np.mean(list(self.user_ratings_dict[neighbor_id].values()))\n",
    "                weighted_sum += sim * (rating - neighbor_mean)\n",
    "                sim_sum += abs(sim)\n",
    "            \n",
    "            if sim_sum > 0:\n",
    "                predicted = target_mean + (weighted_sum / sim_sum)\n",
    "                return max(0.5, min(5.0, predicted))\n",
    "        \n",
    "        return max(0.5, min(5.0, item_ratings['rating'].mean()))\n",
    "    \n",
    "    def calculate_satisfaction(self, user_id, recommendations):\n",
    "        \"\"\"Calculate user satisfaction for recommendations\"\"\"\n",
    "        if user_id not in self.user_ratings_dict:\n",
    "            return 0.0\n",
    "        \n",
    "        predictions = [self.predict_rating(user_id, item) for item in recommendations]\n",
    "        if not predictions:\n",
    "            return 0.0\n",
    "        \n",
    "        avg_rating = np.mean(predictions)\n",
    "        satisfaction = (avg_rating - 0.5) / 4.5\n",
    "        return max(0.0, min(1.0, satisfaction))\n",
    "    \n",
    "    def get_predictions(self, k_candidates=150):\n",
    "        \"\"\"Get predictions for each user\"\"\"\n",
    "        user_predictions = {}\n",
    "        \n",
    "        for user_id in self.state.group_users:\n",
    "            if user_id not in self.user_ratings_dict:\n",
    "                continue\n",
    "            \n",
    "            user_rated = set(self.ratings_df[self.ratings_df['user_id'] == user_id]['item_id'])\n",
    "            candidates = [item for item in self.popular_movies \n",
    "                         if item not in user_rated and item not in self.used_movies]\n",
    "            \n",
    "            predictions = {}\n",
    "            for item in candidates[:k_candidates]:\n",
    "                predictions[item] = self.predict_rating(user_id, item)\n",
    "            \n",
    "            user_predictions[user_id] = predictions\n",
    "        \n",
    "        return user_predictions\n",
    "    \n",
    "    def aggregate_average(self, user_predictions):\n",
    "        \"\"\"Average aggregation\"\"\"\n",
    "        all_items = set()\n",
    "        for preds in user_predictions.values():\n",
    "            all_items.update(preds.keys())\n",
    "        \n",
    "        scores = {}\n",
    "        for item in all_items:\n",
    "            item_scores = [preds[item] for preds in user_predictions.values() if item in preds]\n",
    "            if item_scores:\n",
    "                scores[item] = np.mean(item_scores)\n",
    "        return scores\n",
    "    \n",
    "    def aggregate_least_misery(self, user_predictions):\n",
    "        \"\"\"Least misery aggregation\"\"\"\n",
    "        all_items = set()\n",
    "        for preds in user_predictions.values():\n",
    "            all_items.update(preds.keys())\n",
    "        \n",
    "        scores = {}\n",
    "        for item in all_items:\n",
    "            item_scores = [preds[item] for preds in user_predictions.values() if item in preds]\n",
    "            if item_scores:\n",
    "                scores[item] = min(item_scores)\n",
    "        return scores\n",
    "    \n",
    "    def aggregate_sdaa(self, user_predictions):\n",
    "        \"\"\"SDAA: weighted by cumulative satisfaction\"\"\"\n",
    "        alpha = self.state.get_alpha()\n",
    "        \n",
    "        # Calculate cumulative satisfaction weights\n",
    "        satO = self.state.get_overall_satisfaction()\n",
    "        weights = {}\n",
    "        \n",
    "        if satO:\n",
    "            min_sat = min(satO.values())\n",
    "            max_sat = max(satO.values())\n",
    "            \n",
    "            for user_id in self.state.group_users:\n",
    "                user_sat = satO.get(user_id, 0.5)\n",
    "                # Lower satisfaction gets higher weight\n",
    "                if max_sat > min_sat:\n",
    "                    weights[user_id] = 1.0 - (user_sat - min_sat) / (max_sat - min_sat)\n",
    "                else:\n",
    "                    weights[user_id] = 1.0\n",
    "        else:\n",
    "            weights = {uid: 1.0 for uid in self.state.group_users}\n",
    "        \n",
    "        # Normalize weights\n",
    "        total_w = sum(weights.values())\n",
    "        if total_w > 0:\n",
    "            weights = {uid: w/total_w for uid, w in weights.items()}\n",
    "        \n",
    "        # Aggregate\n",
    "        all_items = set()\n",
    "        for preds in user_predictions.values():\n",
    "            all_items.update(preds.keys())\n",
    "        \n",
    "        scores = {}\n",
    "        for item in all_items:\n",
    "            item_scores = []\n",
    "            weighted_scores = []\n",
    "            \n",
    "            for user_id, preds in user_predictions.items():\n",
    "                if item in preds:\n",
    "                    score = preds[item]\n",
    "                    item_scores.append(score)\n",
    "                    weighted_scores.append(score * weights.get(user_id, 1.0))\n",
    "            \n",
    "            if item_scores:\n",
    "                avg_score = np.mean(item_scores)\n",
    "                weighted_avg = np.mean(weighted_scores) if weighted_scores else avg_score\n",
    "                least_score = min(item_scores)\n",
    "                \n",
    "                # SDAA formula\n",
    "                final_score = (1 - alpha) * weighted_avg + alpha * least_score\n",
    "                scores[item] = final_score\n",
    "        \n",
    "        if self.state.current_round <= 1:\n",
    "            print(f\"    Weights: {{{', '.join([f'{uid}:{w:.2f}' for uid, w in weights.items()])}}}\")\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def aggregate(self, user_predictions):\n",
    "        \"\"\"Choose aggregation method\"\"\"\n",
    "        if self.method == 'Average':\n",
    "            return self.aggregate_average(user_predictions)\n",
    "        elif self.method == 'LeastMisery':\n",
    "            return self.aggregate_least_misery(user_predictions)\n",
    "        else:\n",
    "            return self.aggregate_sdaa(user_predictions)\n",
    "    \n",
    "    def calculate_reward(self):\n",
    "        \"\"\"Calculate R_sd reward\"\"\"\n",
    "        sat = self.state.get_group_satisfaction()\n",
    "        dis = self.state.get_group_disagreement()\n",
    "        \n",
    "        if sat + (1 - dis) == 0:\n",
    "            return 0.0\n",
    "        return 2 * sat * (1 - dis) / (sat + (1 - dis))\n",
    "    \n",
    "    def recommend_round(self, k=5):\n",
    "        \"\"\"One round of recommendation\"\"\"\n",
    "        user_predictions = self.get_predictions()\n",
    "        if not user_predictions:\n",
    "            return [], {}\n",
    "        \n",
    "        scores = self.aggregate(user_predictions)\n",
    "        if not scores:\n",
    "            return [], {}\n",
    "        \n",
    "        # Select top-k\n",
    "        sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        available = [item for item, _ in sorted_items if item not in self.used_movies]\n",
    "        \n",
    "        recommendations = available[:k] if len(available) >= k else [item for item, _ in sorted_items[:k]]\n",
    "        self.used_movies.update(recommendations)\n",
    "        \n",
    "        # Calculate satisfactions\n",
    "        satisfactions = {}\n",
    "        for user_id in self.state.group_users:\n",
    "            satisfactions[user_id] = self.calculate_satisfaction(user_id, recommendations)\n",
    "        \n",
    "        self.state.update_round(recommendations, satisfactions)\n",
    "        return recommendations, satisfactions\n",
    "    \n",
    "    def run(self, num_rounds=10, k=5, verbose=True):\n",
    "        \"\"\"Run sequential recommendations\"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Method: {self.method} | Group: {self.state.group_users}\")\n",
    "            print(f\"{'='*70}\")\n",
    "        \n",
    "        all_recs = []\n",
    "        rewards = []\n",
    "        \n",
    "        for r in range(1, num_rounds + 1):\n",
    "            recs, sats = self.recommend_round(k)\n",
    "            all_recs.append(recs)\n",
    "            reward = self.calculate_reward()\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if verbose and (r <= 3 or r == num_rounds):\n",
    "                sat_str = ', '.join([f'{uid}:{sat:.2f}' for uid, sat in sats.items()])\n",
    "                print(f\"Round {r:2d}: Sats=[{sat_str}], Reward={reward:.2f}\")\n",
    "        \n",
    "        satO = self.state.get_overall_satisfaction()\n",
    "        groupSat = self.state.get_group_satisfaction()\n",
    "        groupDis = self.state.get_group_disagreement()\n",
    "        avgReward = np.mean(rewards)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{'-'*70}\")\n",
    "            satO_str = ', '.join([f'{uid}:{s:.2f}' for uid, s in satO.items()])\n",
    "            print(f\"Final: SatO=[{satO_str}]\")\n",
    "            print(f\"       GroupSat={groupSat:.2f}, GroupDis={groupDis:.2f}, AvgReward={avgReward:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'recommendations': all_recs,\n",
    "            'rewards': rewards,\n",
    "            'group_satisfaction': groupSat,\n",
    "            'group_disagreement': groupDis,\n",
    "            'avg_reward': avgReward,\n",
    "            'user_satisfactions': satO,\n",
    "            'sat_history': self.state.sat_history\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ccea739-1bd9-47d3-a667-909fbfb47b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_methods(group, ratings_df, user_dict, rounds=10):\n",
    "    \"\"\"Compare all three methods\"\"\"\n",
    "    methods = ['Average', 'LeastMisery', 'SDAA']\n",
    "    results = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Comparing three methods: Average vs LeastMisery vs SDAA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for method in methods:\n",
    "        rec = GroupRecommender(group, ratings_df, user_dict, method=method)\n",
    "        results[method] = rec.run(num_rounds=rounds, verbose=True)\n",
    "    \n",
    "    # Summary \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Method':<15} {'GroupSat':<12} {'GroupDis':<12} {'AvgReward':<12}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for method in methods:\n",
    "        res = results[method]\n",
    "        print(f\"{method:<15} {res['group_satisfaction']:<12.3f} \"\n",
    "              f\"{res['group_disagreement']:<12.3f} {res['avg_reward']:<12.3f}\")\n",
    "    \n",
    "    best = max(results.items(), key=lambda x: x[1]['avg_reward'])\n",
    "    print(f\"\\n✓ Best method: {best[0]} (AvgReward={best[1]['avg_reward']:.3f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_results(results, group):\n",
    "    \"\"\"Plot satisfaction trends\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    colors = {'Average': 'blue', 'LeastMisery': 'red', 'SDAA': 'green'}\n",
    "    markers = {'Average': 'o', 'LeastMisery': 's', 'SDAA': '^'}\n",
    "    \n",
    "    # Plot 1: User satisfaction over rounds\n",
    "    ax1 = axes[0]\n",
    "    for method, result in results.items():\n",
    "        sat_history = result['sat_history']\n",
    "        for user_id in group:\n",
    "            if user_id in sat_history:\n",
    "                sats = sat_history[user_id]\n",
    "                rounds = range(1, len(sats) + 1)\n",
    "                ax1.plot(rounds, sats, marker=markers[method], \n",
    "                        label=f'{method}-U{user_id}', \n",
    "                        color=colors[method], alpha=0.7, linewidth=1.5)\n",
    "    \n",
    "    ax1.set_xlabel('Round', fontsize=12)\n",
    "    ax1.set_ylabel('Satisfaction', fontsize=12)\n",
    "    ax1.set_title('User Satisfaction Trends', fontsize=13, fontweight='bold')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Reward trends\n",
    "    ax2 = axes[1]\n",
    "    for method, result in results.items():\n",
    "        rewards = result['rewards']\n",
    "        rounds = range(1, len(rewards) + 1)\n",
    "        ax2.plot(rounds, rewards, marker=markers[method], \n",
    "                label=method, color=colors[method], \n",
    "                linewidth=2.5, markersize=8)\n",
    "    \n",
    "    ax2.set_xlabel('Round', fontsize=12)\n",
    "    ax2.set_ylabel('R_sd Reward', fontsize=12)\n",
    "    ax2.set_title('Reward Trends', fontsize=13, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('part_2_comparison_results.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"\\n✓ Plot saved as 'part_2_comparison_results.png'\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7f437-b623-4cd2-8141-2f649f547cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Comparing three methods: Average vs LeastMisery vs SDAA\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Method: Average | Group: [1, 414, 599]\n",
      "======================================================================\n",
      "Round  1: Sats=[1:1.00, 414:0.84, 599:0.70], Reward=0.76\n",
      "Round  2: Sats=[1:0.99, 414:0.81, 599:0.66], Reward=0.75\n",
      "Round  3: Sats=[1:0.97, 414:0.77, 599:0.58], Reward=0.73\n",
      "Round 10: Sats=[1:0.97, 414:0.79, 599:0.61], Reward=0.71\n",
      "----------------------------------------------------------------------\n",
      "Final: SatO=[1:0.97, 414:0.79, 599:0.62]\n",
      "       GroupSat=0.79, GroupDis=0.35, AvgReward=0.72\n",
      "\n",
      "======================================================================\n",
      "Method: LeastMisery | Group: [1, 414, 599]\n",
      "======================================================================\n",
      "Round  1: Sats=[1:1.00, 414:0.84, 599:0.70], Reward=0.76\n",
      "Round  2: Sats=[1:0.99, 414:0.81, 599:0.66], Reward=0.75\n",
      "Round  3: Sats=[1:0.97, 414:0.77, 599:0.58], Reward=0.73\n"
     ]
    }
   ],
   "source": [
    "# Run comparison\n",
    "results = compare_methods(test_group, ratings_df, user_ratings_dict, rounds=10)\n",
    "\n",
    "# Plot results\n",
    "plot_results(results, test_group)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Testing ends\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef15713-f2cc-41ad-991a-507952d5968d",
   "metadata": {},
   "source": [
    "\n",
    "# Step 3: Why SDAA Works for Sequential Group Recommendations (6 points)\n",
    "\n",
    "## Baseline Methods from SQUIRREL\n",
    "\n",
    "We compared three aggregation methods from the SQUIRREL framework:\n",
    "\n",
    "**Average:** Treats all users equally every round → Minority users (like User 599) stay consistently unsatisfied\n",
    "\n",
    "**Least Misery:** Always focuses on the pickiest user → Lower overall satisfaction\n",
    "\n",
    "**SDAA:** Dynamically adjusts based on cumulative satisfaction history → Balances fairness and satisfaction\n",
    "\n",
    "---\n",
    "\n",
    "## How SDAA Solves the Problem\n",
    "\n",
    "SDAA tracks cumulative satisfaction history and dynamically adjusts user weights. Users with lower overall satisfaction get higher weights in future rounds, ensuring long-term fairness. The aggregation blends weighted average and least misery using alpha:\n",
    "\n",
    "$$\\text{score} = (1-\\alpha) \\cdot \\text{weighted\\_avg} + \\alpha \\cdot \\text{least}$$\n",
    "\n",
    "**Example from our experiment:** \n",
    "After Round 1, User 599 had lowest satisfaction (0.70). In Round 2, SDAA gave User 599 weight 0.65 vs User 1's weight 0.00, improving User 599's satisfaction to 0.68.\n",
    "\n",
    "---\n",
    "\n",
    "## Testing Results\n",
    "\n",
    "| Metric              | Average | LeastMisery | SDAA    |\n",
    "|---------------------|---------|-------------|---------|\n",
    "| Group Satisfaction  | 0.791   | 0.789       | **0.791** |\n",
    "| Group Disagreement  | 0.350   | 0.350       | **0.344** |\n",
    "| Average Reward (R_sd) | 0.723   | 0.723       | **0.736** |\n",
    "\n",
    "- SDAA maintains the same overall satisfaction as Average (0.791)\n",
    "- SDAA reduces disagreement by 1.7% (0.344 vs 0.350) → More balanced\n",
    "- SDAA achieves 1.8% higher R_sd reward (0.736 vs 0.723) → Best performance\n",
    "\n",
    "The visualization confirms SDAA (green line) maintains the highest reward throughout all 10 rounds.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Among the three SQUIRREL methods we implemented, SDAA performs best because:\n",
    "\n",
    "**Memory:** Tracks cumulative satisfaction, not just current round  \n",
    "**Adaptivity:** Dynamically adjusts weights based on history  \n",
    "**Balance:** Blends equality (Average) with inclusivity (Least Misery)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7068a-2a14-4586-8a42-f4d9e84ae039",
   "metadata": {},
   "source": [
    "# Step 4: Presentation (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9b51e6-3987-455c-81ee-be436dcf1eb1",
   "metadata": {},
   "source": [
    "5 slides (Done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a10c4-537f-4ac1-b706-70af893927c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
